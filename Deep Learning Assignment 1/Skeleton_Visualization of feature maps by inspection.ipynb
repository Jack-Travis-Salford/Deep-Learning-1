{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 11: Visualization of feature maps by inspection Skeleton for you to complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining the  function that creates a the model.\n",
    "\n",
    "It applies the model to img_array, retrieves layer_no and then displays in no_cols per row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_maps(network,img_array,layer_no, no_cols=4):\n",
    "    \n",
    "    layer_outputs = [layer.output for layer in network.layers]\n",
    "    activations_model = models.Model(inputs=network.input, outputs = layer_outputs)\n",
    "    activations = activations_model.predict(img_array)\n",
    "    \n",
    "    # collect images in list\n",
    "\n",
    "    no_maps = np.shape(activations[layer_no])[-1]    \n",
    "    print(\"layer %d, no maps %d\"%(layer_no, no_maps))\n",
    "    fmaps =activations[layer_no]\n",
    "    img_list = []\n",
    "    for i in range(no_maps):\n",
    "        img = fmaps[0,:,:,i]\n",
    "        img_list.append(img)\n",
    "\n",
    "#    display images in no_cols\n",
    "    \n",
    "    no_rows = no_maps // no_cols\n",
    "    i = 0\n",
    "    for row in range(no_rows):\n",
    "        fig, axs = plt.subplots(1, no_cols,figsize=(18,18))\n",
    "        for col in range(no_cols):\n",
    "            axs[col].axis('off')\n",
    "            axs[col].imshow(img_list[i])\n",
    "            axs[col].set_title(str(i))\n",
    "            i +=1\n",
    "            \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. function to get the index for a layer given name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(network,layer_name):\n",
    "    layer_names = [layer.name for layer in network.layers]\n",
    "    return layer_names.index(layer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizing feature maps for MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1929 images belonging to 10 classes.\n",
      "Found 548 images belonging to 10 classes.\n",
      "Found 287 images belonging to 10 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2359808   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,079,626\n",
      "Trainable params: 17,079,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "97/97 [==============================] - 13s 42ms/step - loss: 1.6808 - acc: 0.4049 - val_loss: 0.4767 - val_acc: 0.8303\n",
      "Epoch 2/30\n",
      "97/97 [==============================] - 3s 33ms/step - loss: 0.3629 - acc: 0.8751 - val_loss: 0.1091 - val_acc: 0.9745\n",
      "Epoch 3/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.1364 - acc: 0.9611 - val_loss: 0.0448 - val_acc: 0.9927\n",
      "Epoch 4/30\n",
      "97/97 [==============================] - 3s 33ms/step - loss: 0.0777 - acc: 0.9767 - val_loss: 0.0471 - val_acc: 0.9854\n",
      "Epoch 5/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0487 - acc: 0.9865 - val_loss: 0.0457 - val_acc: 0.9836\n",
      "Epoch 6/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0380 - acc: 0.9912 - val_loss: 0.0413 - val_acc: 0.9854\n",
      "Epoch 7/30\n",
      "97/97 [==============================] - 3s 33ms/step - loss: 0.0311 - acc: 0.9902 - val_loss: 0.0611 - val_acc: 0.9763\n",
      "Epoch 8/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0316 - acc: 0.9912 - val_loss: 0.0269 - val_acc: 0.9927\n",
      "Epoch 9/30\n",
      "97/97 [==============================] - 3s 33ms/step - loss: 0.0260 - acc: 0.9938 - val_loss: 0.0190 - val_acc: 0.9945\n",
      "Epoch 10/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0155 - acc: 0.9943 - val_loss: 0.0307 - val_acc: 0.9909\n",
      "Epoch 11/30\n",
      "97/97 [==============================] - 3s 34ms/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0135 - val_acc: 0.9945\n",
      "Epoch 12/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0202 - acc: 0.9938 - val_loss: 0.0290 - val_acc: 0.9909\n",
      "Epoch 13/30\n",
      "97/97 [==============================] - 3s 33ms/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0135 - val_acc: 0.9945\n",
      "Epoch 14/30\n",
      "97/97 [==============================] - 3s 33ms/step - loss: 0.0052 - acc: 0.9995 - val_loss: 0.0152 - val_acc: 0.9964\n",
      "Epoch 15/30\n",
      "97/97 [==============================] - 3s 33ms/step - loss: 0.0073 - acc: 0.9969 - val_loss: 0.0725 - val_acc: 0.9763\n",
      "Epoch 16/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0168 - val_acc: 0.9945\n",
      "Epoch 17/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 0.0166 - acc: 0.9948 - val_loss: 0.0322 - val_acc: 0.9872\n",
      "Epoch 18/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 0.0070 - acc: 0.9974 - val_loss: 0.0153 - val_acc: 0.9964\n",
      "Epoch 19/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0156 - val_acc: 0.9945\n",
      "Epoch 20/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0049 - acc: 0.9979 - val_loss: 0.0199 - val_acc: 0.9927\n",
      "Epoch 21/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0215 - val_acc: 0.9927\n",
      "Epoch 22/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 8.7570e-04 - acc: 1.0000 - val_loss: 0.0233 - val_acc: 0.9945\n",
      "Epoch 23/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 7.2896e-04 - acc: 1.0000 - val_loss: 0.0194 - val_acc: 0.9927\n",
      "Epoch 24/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.0017 - acc: 0.9990 - val_loss: 0.0237 - val_acc: 0.9945\n",
      "Epoch 25/30\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 8.2177e-04 - acc: 1.0000 - val_loss: 0.0232 - val_acc: 0.9964\n",
      "Epoch 26/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 3.5439e-04 - acc: 1.0000 - val_loss: 0.0255 - val_acc: 0.9945\n",
      "Epoch 27/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 0.0098 - acc: 0.9979 - val_loss: 0.0272 - val_acc: 0.9964\n",
      "Epoch 28/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0305 - val_acc: 0.9836\n",
      "Epoch 29/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 0.0282 - val_acc: 0.9964\n",
      "Epoch 30/30\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0285 - val_acc: 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: asl2_network\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: asl2_network\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2359808   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,079,626\n",
      "Trainable params: 17,079,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.losses\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "base_dir = 'C:\\\\--Mine\\\\My Files\\\\Uni\\\\Y3\\\\Deep Learning\\\\assignment\\\\signlang'\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "val_dir = os.path.join(base_dir,'val')\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,target_size=(100,100),batch_size=20, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir,target_size=(100,100),batch_size=20, class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,target_size=(100,100),batch_size=20, class_mode='categorical', shuffle=False)\n",
    "\n",
    "# Run the model you developed previously and save\n",
    "# it using save_model(\"fashion_mnist_model\").  Then load it here:\n",
    "\n",
    "from keras import models, layers\n",
    "\n",
    "\n",
    "from keras.applications import VGG16\n",
    "vgg_base = VGG16(weights='imagenet', include_top=False,input_shape=(100,100,3))\n",
    "network = models.Sequential()\n",
    "network.add(vgg_base)\n",
    "network.add(layers.Flatten())\n",
    "\n",
    "network.add(layers.Dropout(0.5))\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(10,activation='sigmoid'))\n",
    "network.summary()\n",
    "from keras import optimizers\n",
    "#network.compile(loss='categorical_crossentropy', optimizer= optimizers.RMSprop(learning_rate=1e-5), metrics=['acc'] )\n",
    "\n",
    "network.compile(loss='categorical_crossentropy', optimizer= optimizers.Adam(learning_rate=1e-5), metrics=['acc'] )\n",
    "\n",
    "\n",
    "hist = network.fit(train_generator, steps_per_epoch=97, epochs=30,\n",
    "                   validation_data=val_generator, validation_steps=28)\n",
    "\n",
    "network.save('asl2_network')\n",
    "network = models.load_model(\"asl2_network\")\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the data and pick an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.preprocessing.image' has no attribute 'img_to_array'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_7248\\3198629746.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mimg1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata_batch\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mimg1_array\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimg_to_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[0mimg1_array\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg1_array\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'keras.preprocessing.image' has no attribute 'img_to_array'"
     ]
    }
   ],
   "source": [
    "for data_batch, label_batch in train_generator:\n",
    "    break\n",
    "\n",
    "img1 = data_batch[0]\n",
    "img1_array = image.img_to_array(img1)\n",
    "img1_array = np.expand_dims(img1_array, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pick a layer and display its feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???LAYERNAME??? =  You can see the names when you use network.summary()\n",
    "\n",
    "layer_no = get_index(network, ???LAYERNAME???)\n",
    "display_feature_maps(network, img1_array,layer_no,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a later layer and display its feture maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_name_\n",
    "layer_no = get_index(network,???TRY ANOTHER LAYER???)\n",
    "display_feature_maps(network, img1_array,layer_no,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
